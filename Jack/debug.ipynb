{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "################################ debug functions ###############################\n",
    "def debug(*argv):\n",
    "    if True:\n",
    "        print(*argv)\n",
    "\n",
    "\n",
    "def debug_dict(title, my_dict):\n",
    "    debug(title + ' = ')\n",
    "    for key in my_dict:\n",
    "        debug(key, my_dict[key])\n",
    "    debug()\n",
    "\n",
    "\n",
    "def debug_matrix(title, matrix):\n",
    "    debug(title + ' = ')\n",
    "    for line in matrix:\n",
    "        debug(line)\n",
    "    debug()\n",
    "\n",
    "\n",
    "############################## train data functions ############################\n",
    "def get_vocabulary(strategy_instance):\n",
    "    # generate a sorted list contains all words in all samples\n",
    "    word_set = set()\n",
    "    for sample in strategy_instance.class0:\n",
    "        for word in sample:\n",
    "            word_set.add(word)\n",
    "    for sample in strategy_instance.class1:\n",
    "        for word in sample:\n",
    "            word_set.add(word)\n",
    "    vocabulary = sorted(list(word_set))\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def get_feature_vector(input_vector, vocabulary):\n",
    "    dim = len(vocabulary)\n",
    "    output_vector = [0 for i in range(dim)]\n",
    "    for word in input_vector:\n",
    "        try:\n",
    "            i = vocabulary.index(word)\n",
    "            output_vector[i] = 1\n",
    "        except ValueError:\n",
    "            pass    # remains 0 when the word is not in word_table\n",
    "    return output_vector\n",
    "\n",
    "\n",
    "def get_feature_matrix(input_matrix, vocabulary):\n",
    "    # arguments: \n",
    "    #   input_matrix = [ input_list = [word]]\n",
    "    # return:\n",
    "    #   output_matrix = [ output_list = [existence of word]]\n",
    "    #   based on vocabulary\n",
    "    #\n",
    "    output_matrix = list()\n",
    "    for input_vector in input_matrix:\n",
    "        output_vector = get_feature_vector(input_vector, vocabulary)\n",
    "        output_matrix.append(output_vector)\n",
    "    return output_matrix\n",
    "\n",
    "\n",
    "def get_x_train(strategy_instance, vocabulary):\n",
    "    word_matrix = strategy_instance.class0 + strategy_instance.class1\n",
    "    x_matrix = get_feature_matrix(word_matrix, vocabulary)\n",
    "    x_train = np.array(x_matrix)\n",
    "    return x_train\n",
    "\n",
    "\n",
    "def get_y_train(strategy_instance):\n",
    "    len0 = len(strategy_instance.class0)\n",
    "    len1 = len(strategy_instance.class1)\n",
    "    y_list = [0 for i in range(len0)] + [1 for i in range(len1)]\n",
    "    y_train = np.array(y_list)\n",
    "    return y_train\n",
    "\n",
    "\n",
    "def get_prediction(clf, file_path, vocabulary):\n",
    "    # print out the index of wrong answer\n",
    "    # generate feature\n",
    "    with open(file_path,'r') as fh:\n",
    "        word_matrix=[line.strip().split(' ') for line in fh]\n",
    "    x_matrix = get_feature_matrix(word_matrix, vocabulary)\n",
    "    x_train = np.array(x_matrix)\n",
    "    prediction = clf.predict(x_train)\n",
    "    decision_function = clf.decision_function(x_train)\n",
    "    return prediction, decision_function\n",
    "\n",
    "\n",
    "############################# modify file functions ############################\n",
    "def get_weight_dict(weight_list, vocabulary):\n",
    "    weight_dict = defaultdict(float)\n",
    "    for i in range(len(vocabulary)):\n",
    "        weight_dict[vocabulary[i]] = weight_list[i]\n",
    "    return weight_dict\n",
    "\n",
    "\n",
    "def read_to_matrix(file_path):\n",
    "    # read txt file to matrix\n",
    "    with open(file_path,'r') as fh:\n",
    "        data_matrix=[line.strip().split(' ') for line in fh]\n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "def get_class1_vector(input_vector, weight_dict):\n",
    "    # remove duplicated words\n",
    "    cleaned_vector = list(set(input_vector))\n",
    "    # sort based on weight (from class 1 to class 0)\n",
    "    combined_vector = []\n",
    "    for word in cleaned_vector:\n",
    "        combined_vector.append((word, weight_dict[word]))\n",
    "    sorted_vector = sorted(combined_vector,key=lambda l:l[1], reverse=True)\n",
    "    # unzip and generate output_vector\n",
    "    output_vector = []\n",
    "    for pair in sorted_vector:\n",
    "        output_vector.append(pair[0])\n",
    "    return output_vector\n",
    "\n",
    "\n",
    "def get_class0_vocabulary(input_vector, weight_dict, vocabulary):\n",
    "    # remove words in vocabulary which are in input_vector as well\n",
    "    cleaned_vector = list(set(vocabulary) - set(input_vector))\n",
    "    # sort based on weight (from class 0 to class 1)\n",
    "    combined_vector = []\n",
    "    for word in cleaned_vector:\n",
    "        combined_vector.append((word, weight_dict[word]))\n",
    "    sorted_vector = sorted(combined_vector,key=lambda l:l[1])\n",
    "    # unzip and generate output_vector\n",
    "    output_vector = []\n",
    "    for pair in sorted_vector:\n",
    "        output_vector.append(pair[0])\n",
    "    return output_vector\n",
    "\n",
    "\n",
    "def get_modified_vector(input_vector, weight_dict, vocabulary, n):\n",
    "    # change class1_vector base on class0_vocabulary\n",
    "    class1_vector = get_class1_vector(input_vector, weight_dict)\n",
    "    class0_vocabulary =\\\n",
    "            get_class0_vocabulary(input_vector, weight_dict, vocabulary)\n",
    "    i_1 = 0     # counter of class1_vector\n",
    "    i_0 = 0     # counter of class0_vocabulary\n",
    "    output_vector = []\n",
    "    # perform n time distinct change\n",
    "    for i in range(n):\n",
    "        # eg.   how can we get from class1 to class0\n",
    "        # class1_vector[i_1]               weight_sum\n",
    "        #           class0_vocabulary[i_0]              choice\n",
    "        #      1           -2                 <0    add class0_vocabulary[i_0]\n",
    "        #      1           -0.5               >0    rm class1_vector[i_1]\n",
    "        #      1           0.5                >0    rm  class1_vector[i_1]\n",
    "        #      1           2                  >0    class1_vector[i_1]\n",
    "        #      -1          -2                 <0    add class0_vocabulary[i_0]\n",
    "        #      -1          -0.5               <0    add class0_vocabulary[i_0]\n",
    "        #      -1          0.5                <0    add class0_vocabulary[i_0]\n",
    "        #      -1          2                  >0    rm  class1_vector[i_1]\n",
    "        #\n",
    "        weight_sum = weight_dict[class1_vector[i_1]]\\\n",
    "                     + weight_dict[class0_vocabulary[i_0]]\n",
    "        if weight_sum < 0:\n",
    "            # rm  class1_vector[i_1]\n",
    "            i_1 += 1\n",
    "        else:\n",
    "            # add class0_vocabulary[i_0]\n",
    "            output_vector.append(class0_vocabulary[i_0])\n",
    "            i_0 += 1\n",
    "    # add remained items to output_vector\n",
    "    for i in range(i_1, len(class1_vector)):\n",
    "        output_vector.append(class1_vector[i])\n",
    "    return output_vector\n",
    "\n",
    "\n",
    "def get_modified_matrix(input_matrix, weight_dict, vocabulary, n):\n",
    "    # change from class1 to class0\n",
    "    output_matrix = []\n",
    "    for input_vector in input_matrix:\n",
    "        output_vector = get_modified_vector(\\\n",
    "                input_vector, weight_dict, vocabulary, n)\n",
    "        output_matrix.append(output_vector.copy())\n",
    "    return output_matrix\n",
    "\n",
    "\n",
    "def write_to_file(input_matrix ,file_path):\n",
    "    with open(file_path, 'w') as fh:\n",
    "        for input_vector in input_matrix:\n",
    "            line = ' '.join(input_vector) + '\\n'\n",
    "            fh.write(line)\n",
    "\n",
    "\n",
    "################################ debug functions ###############################\n",
    "def show_test_result(clf, vocabulary):\n",
    "    dim = len(vocabulary)\n",
    "    # get prediction\n",
    "    prediction0, decision_function0 = get_prediction(clf, './class-0.txt', vocabulary)\n",
    "    prediction1, decision_function1 = get_prediction(clf, './class-1.txt', vocabulary)\n",
    "    prediction_test,decision_function_test = get_prediction(clf, './test_data.txt', vocabulary)\n",
    "    prediction_mod, decision_function_mod = get_prediction(clf, './modified_data.txt', vocabulary)\n",
    "    # calculate success rate\n",
    "    rate0 = prediction0.tolist().count(0) / prediction0.shape[0] * 100\n",
    "    rate1 = prediction1.tolist().count(1) / prediction1.shape[0] * 100\n",
    "    rate_test = prediction_test.tolist().count(1)/prediction_test.shape[0] * 100\n",
    "    rate_mod = prediction_mod.tolist().count(0)/prediction_mod.shape[0] * 100\n",
    "    # print test result\n",
    "    print('########################## Test Result ############################')\n",
    "    print(str(clf))\n",
    "    print('############################ Summery ##############################')\n",
    "    print('class-0 Success Rate = ' + str(rate0))\n",
    "    print('class-1 Success Rate = ' + str(rate1))\n",
    "    print('test_data Success Rate = ' + str(rate_test))\n",
    "    print('modified_data Success Rate = ' + str(rate_mod))\n",
    "    print('############################# Detail ##############################')\n",
    "    # print('class-0 prediction =\\n' + str(prediction0))\n",
    "    # print('class-1 prediction =\\n' + str(prediction1))\n",
    "    print('test_data prediction =\\n' + str(prediction_test))\n",
    "    # print('modiefied_data prediction =\\n' + str(prediction_mod))\n",
    "    # print('class-0 decision_function =\\n' + str(decision_function0))\n",
    "    # print('class-1 decision_function =\\n' + str(decision_function1))\n",
    "    print('test_data decision_function =\\n' + str(decision_function_test))\n",
    "    print('test_data sum = ', sum(decision_function_test))\n",
    "    print('modiefied_data decision_function =\\n' + str(decision_function_mod))\n",
    "    print('modiefied_data sum = ', sum(decision_function_mod))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################## Test Result ############################\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "############################ Summery ##############################\n",
      "class-0 Success Rate = 100.0\n",
      "class-1 Success Rate = 100.0\n",
      "test_data Success Rate = 50.0\n",
      "modified_data Success Rate = 100.0\n",
      "############################# Detail ##############################\n",
      "test_data prediction =\n",
      "[1 0 1 0]\n",
      "test_data decision_function =\n",
      "[ 0.60006497 -0.35189157  0.85825105 -0.5931515 ]\n",
      "test_data sum =  0.5132729543917629\n",
      "modiefied_data decision_function =\n",
      "[-2.74305907 -1.94117322 -2.4186221  -3.78403004]\n",
      "modiefied_data sum =  -10.886884434497002\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-2-a86a2b3a5241>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a86a2b3a5241>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    return strategy_instance ## NOTE: You are required to return the instance of this class.\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "strategy_instance=helper.strategy()\n",
    "\n",
    "########################### define parameter ###########################\n",
    "test_data='./test_data.txt'\n",
    "n = 20     # the number of distinct words that can be modified\n",
    "parameters={'gamma': 'auto',\n",
    "            'C': 1.0,\n",
    "            'kernel': 'linear',\n",
    "            'degree': 3,\n",
    "            'coef0': 0.0\n",
    "            }\n",
    "# gamma : float, optional (default='auto') 2^-15 ~ 2^3\n",
    "#     Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
    "#     If gamma is 'auto' then 1/n_features will be used instead.\n",
    "# C : float, optional (default=1.0) 2^-5 ~ 2^15\n",
    "#     Penalty parameter C of the error term.\n",
    "# kernel : string, optional (default='rbf')\n",
    "#     Specifies the kernel type to be used in the algorithm.\n",
    "#     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
    "#     a callable.\n",
    "#     If none is given, 'rbf' will be used. If a callable is given it is\n",
    "#     used to pre-compute the kernel matrix from data matrices; that matrix\n",
    "#     should be an array of shape ``(n_samples, n_samples)``.\n",
    "# degree : int, optional (default=3)\n",
    "#     Degree of the polynomial kernel function ('poly').\n",
    "#     Ignored by all other kernels.\n",
    "# coef0 : float, optional (default=0.0)\n",
    "#    Independent term in kernel function.\n",
    "#    It is only significant in 'poly' and 'sigmoid'.\n",
    "\n",
    "############################# train data ###############################\n",
    "# debug_matrix('class0', strategy_instance.class0)\n",
    "# debug_matrix('class1', strategy_instance.class1)\n",
    "# get vocabulary\n",
    "vocabulary = get_vocabulary(strategy_instance)\n",
    "# debug('vocabulary =\\n', vocabulary)\n",
    "# get y_train\n",
    "y_train = get_y_train(strategy_instance)\n",
    "# debug('y_train =\\n', y_train)\n",
    "# get x_train\n",
    "x_train = get_x_train(strategy_instance, vocabulary)\n",
    "# debug('x_train =\\n', x_train)\n",
    "# training\n",
    "clf = strategy_instance.train_svm(parameters, x_train, y_train)\n",
    "\n",
    "############################# modify file ##############################\n",
    "# read test_data.txt\n",
    "test_data_matrix = read_to_matrix(test_data)\n",
    "# debug_matrix('test_data_matrix', test_data_matrix)\n",
    "# get weight_list\n",
    "weight_list = clf.coef_.tolist()[0]\n",
    "# debug('weight_list =\\n', weight_list)\n",
    "# get weight_dict\n",
    "weight_dict = get_weight_dict(weight_list, vocabulary)\n",
    "# debug_dict('weight_dict', weight_dict)\n",
    "# get modified matrix\n",
    "modified_data_matrix =\\\n",
    "        get_modified_matrix(test_data_matrix, weight_dict, vocabulary, n)\n",
    "# debug_matrix('modified_data_matrix', modified_data_matrix)\n",
    "# write to modified_data\n",
    "modified_data='./modified_data.txt'\n",
    "write_to_file(modified_data_matrix ,modified_data)\n",
    "\n",
    "################################## test  ###################################\n",
    "# Check that the modified text is within the modification limits.\n",
    "assert strategy_instance.check_data(test_data, modified_data)\n",
    "# Show test result\n",
    "show_test_result(clf, vocabulary)\n",
    "return strategy_instance ## NOTE: You are required to return the instance of this class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn import svm, datasets\n",
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> iris = datasets.load_iris()\n",
    "print(iris.data)\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': None,\n",
       " 'error_score': 'raise',\n",
       " 'estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'estimator__C': 1.0,\n",
       " 'estimator__cache_size': 200,\n",
       " 'estimator__class_weight': None,\n",
       " 'estimator__coef0': 0.0,\n",
       " 'estimator__decision_function_shape': 'ovr',\n",
       " 'estimator__degree': 3,\n",
       " 'estimator__gamma': 'auto',\n",
       " 'estimator__kernel': 'rbf',\n",
       " 'estimator__max_iter': -1,\n",
       " 'estimator__probability': False,\n",
       " 'estimator__random_state': None,\n",
       " 'estimator__shrinking': True,\n",
       " 'estimator__tol': 0.001,\n",
       " 'estimator__verbose': False,\n",
       " 'fit_params': None,\n",
       " 'iid': True,\n",
       " 'n_jobs': 1,\n",
       " 'param_grid': {'C': [1, 10], 'kernel': ['linear']},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'refit': True,\n",
       " 'return_train_score': 'warn',\n",
       " 'scoring': None,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> parameters = {'kernel':['linear'], 'C':[1, 10]}\n",
    ">>> svc = svm.SVC()\n",
    ">>> clf = GridSearchCV(svc, parameters)\n",
    ">>> clf.fit(iris.data, iris.target)\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_start = strategy_instance.train_svm(parameters, X,y)\n",
    "param_range = np.arange(0.001,1,0.01)\n",
    "param_grid = [{'C': param_range, 'kernel': ['linear']}]\n",
    "grid = GridSearchCV(clf_start, param_grid)\n",
    "grid.fit(X,y)\n",
    "clf = grid.best_estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
